{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Filtering the vocabulary of the mT5 SentencePiece tokenizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0febbb9787d847f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, T5TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_id = \"google/mt5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, force_download=True, legacy=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc0151489fe7d981"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load previously saved dataset and tokenize it"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26a9528ae83f736e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "german_ds = load_from_disk(\"german_ds\")\n",
    "\n",
    "def convert_to_tokens(source):\n",
    "    return tokenizer(source[\"text\"])\n",
    "\n",
    "tokenized_ds = german_ds.map(convert_to_tokens, remove_columns=['text'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b870217df76fdd33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_ids = tokenized_ds[\"train\"][\"input_ids\"]\n",
    "input_ids.extend(tokenized_ds[\"test\"][\"input_ids\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e5e648158585b7d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get a flat list of all input ids from the dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4761105071873dc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "full_vocabulary = tokenizer.get_vocab()\n",
    "token_list = input_ids\n",
    "token_list = list(itertools.chain(*token_list))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb598ffdf8a977e4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(token_list)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b98aa0585c40f552"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set the target vocabulary size and sort the list of token ids by frequency "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4353a9defa7b22d7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "target_vocab_size = 32000 # T5 vocabulary size\n",
    "counts = collections.Counter(token_list)\n",
    "sorted_list = sorted(token_list, key=counts.get, reverse=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f11fa3cd057e3196"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(sorted_list[0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "caaafe60717e4426"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get rid of duplicate ids so that the result is a list of all unique token ids sorted by frequency."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42c814cb4fd4de0c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seen = set()\n",
    "seen_add = seen.add\n",
    "filtered_sorted_list = [x for x in sorted_list if (not (x in seen or seen_add(x))) and x > 258]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b0ba2f7ea21d898"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Take the first 32.000 token ids from the list."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bdd1f706029b3a01"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filtered_vocab_ids = filtered_sorted_list[:32000]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4afc3b5ac97054dc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the SentencePiece model from the tokenizer, which is needed to get the corresponding sentencepiece for each token id"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77c5e8f7ede325a4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sentencepiece import sentencepiece_model_pb2 as sp_model \n",
    "\n",
    "def load_spm_protopub():\n",
    "    m = sp_model.ModelProto()\n",
    "    m.ParseFromString(open(tokenizer.vocab_file, 'rb').read())\n",
    "    return m\n",
    "\n",
    "m = load_spm_protopub()\n",
    "\n",
    "# There are some reserved places for special tokens up until index 258\n",
    "for i, piece in enumerate(m.pieces[:320]):\n",
    "    print(i, piece.piece)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72d90fcf5b82065f"
  },
  {
   "cell_type": "markdown",
   "source": [
    " Find the sentencepieces that should be kept based on the list of filtered token ids"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b81c669f4883ccb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "kept_pieces, i = [], len(m.pieces) - 1\n",
    "\n",
    "progress_bar = tqdm(total=len(m.pieces))\n",
    "while len(m.pieces):\n",
    "    piece = m.pieces.pop()\n",
    "    if i < 259 or i in filtered_vocab_ids:\n",
    "        kept_pieces.append(piece)\n",
    "    i -= 1\n",
    "    progress_bar.update(1)\n",
    "kept_pieces = list(reversed(kept_pieces))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe5a20c3aab4c60"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get the piece and the score for each kept sentencepiece"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4e6b66adb6c6328"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "m = load_spm_protopub()\n",
    "\n",
    "for i in range(len(m.pieces) - len(kept_pieces)): _ = m.pieces.pop()\n",
    "print(len(m.pieces))\n",
    "\n",
    "i = 0\n",
    "for p in m.pieces:\n",
    "    p.piece = kept_pieces[i].piece\n",
    "    p.score = kept_pieces[i].score\n",
    "    i += 1\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53aba441321db5b1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(m.pieces)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ab5276348a162b1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save the trimmed sentencepiece model "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a52ab2d77c6b1e5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"spiece.model\", \"wb\") as f:\n",
    "    f.write(m.SerializeToString())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "660ba0747cc2f2d6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save a file containing the ids of the tokens that are staying in the vocabulary. This is needed to later on adjust the embedding layer of a model that is to be trained with the trimmed vocabulary."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61bcece6cbbff701"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "kept_ids = sorted(list(set(filtered_vocab_ids).union(set(range(259)))))\n",
    "print(len(kept_ids))\n",
    "with open(\"kept_ids.json\", 'w') as f:\n",
    "    json.dump(kept_ids, f)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3a6c60e78a38b0a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Construct a new tokenizer from the trimmed sentencepiece model using T5TokenizerFast and save it"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7717fc75694db5ea"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_tokenizer = T5TokenizerFast(vocab_file=\"spiece.model\", extra_ids=0, legacy=False)\n",
    "tokenizer_path = \"filtered_tokenizer\"\n",
    "new_tokenizer.save_pretrained(tokenizer_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6534a2761de0242"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
