{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    " ## Perform the Span-MLM training step of Vocabulary Transfer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c20fda44f5caf99"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the new and old tokenizer as well as mT5"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "184b9b49cb872ad3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72b93b3c2d458db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model_id = \"google/mT5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"german_tokenizer\")\n",
    "tokenizer_old = AutoTokenizer.from_pretrained(model_id, legacy=False, force_download=True, device_map={\"\":0})\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initialize the embeddings for the new vocabulary based on the Vocabulary Transfer proposed by Mosin et al.(https://doi.org/10.1016/j.artint.2023.103860)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "471db5f0841f14d3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from vocabulary_transfer import initialize_embeddings\n",
    "\n",
    "model = initialize_embeddings(model=model, tokenizer_old=tokenizer_old, tokenizer_new=tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the pre-tokenized noised dataset for span-MLM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f016725fdc14c9ce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611df1f2-9961-43fd-8940-68c7693437e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "data_path = \"german_ds_smlm_noised\"\n",
    "span_mlm_ds = load_from_disk(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9016a56b-e590-4e2a-a70d-61424f3d91da",
   "metadata": {},
   "outputs": [],
   "source": [
    "span_mlm_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setup DataCollator, optimizer and Training Arguments"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95e20138b964fbde"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868407df83577577",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, Adafactor\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "optimizer = Adafactor(model.parameters(), lr=1e-3, scale_parameter=False, relative_step=False,\n",
    "                      clip_threshold=1.0, decay_rate=0.0)\n",
    "\n",
    "output_dir = \"mT5-small_vt_smlm\"\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    per_device_train_batch_size=16,\n",
    "    warmup_ratio=0.1,\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=1,\n",
    "    logging_dir=f'logs',\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    optim=\"adafactor\",\n",
    "    lr_scheduler_type=\"constant\",\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=span_mlm_ds[\"train\"].select(range(100000)), # select only part of the dataset so shorten training time\n",
    "    optimizers=(optimizer, None)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Start training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71e71e670b4ad64f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b7b25481717975",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536519ec-dcdc-47ab-93c0-330f73198a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "history = pd.DataFrame(trainer.state.log_history)\n",
    "\n",
    "plt.plot(history['loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Push the model to a repository in the Hugging Face Hub"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ff5b2bee94da48a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# set Hugging Face Hub token here to push to hub\n",
    "token = ''\n",
    "\n",
    "trainer.model.push_to_hub(f'{dir}', save_embedding_layers=True, token=token, private=True)\n",
    "tokenizer.push_to_hub(f'{dir}', token=token, private=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f37bdac057b0e746"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The resulting model can be Fine-Tuned with LoRA with the [finetuning_evaluation_pipeline.ipynb](finetuning_evaluation_pipeline.ipynb) Notebook."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "192d353e655ae6b6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
